{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyMUcdWGvDjfzFQHLP0hD//W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/runqingz/CSC2515YOLOPedestrianDetection/blob/main/Yolor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4sWMP_L5dp6"
      },
      "source": [
        "**Dependencies for preparing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o99R2qTl9UQR",
        "outputId": "929b8650-d802-47a0-a27c-c830ffec8369"
      },
      "source": [
        "!pip install lxml\n",
        "!pip install tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iURXJO36bYS"
      },
      "source": [
        "**Unzip Data.zip**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqLiptYp3Knc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995b65c1-437f-45c6-d0ec-c6e25b8b637b"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"/content/Data.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zipfile:\n",
        "  zipfile.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n",
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycoaAIARMSkS"
      },
      "source": [
        "from lxml import etree as ET\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from IPython.display import Image\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqp4Z2ti6WBM"
      },
      "source": [
        "**Helper Function: Convert XML to YOLO info dict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNS8S9RO8PUT"
      },
      "source": [
        "# Function to get the data from XML Annotation\n",
        "def extract_info_from_xml(xml_file):\n",
        "    root = ET.parse(xml_file).getroot()\n",
        "    \n",
        "    # Initialise the info dict \n",
        "    info_dict = {}\n",
        "    info_dict['bboxes'] = []\n",
        "\n",
        "    # Parse the XML Tree\n",
        "    for elem in root:\n",
        "        # Get the file name \n",
        "        if elem.tag == \"filename\":\n",
        "            info_dict['filename'] = elem.text\n",
        "            \n",
        "        # Get the image size\n",
        "        elif elem.tag == \"size\":\n",
        "            image_size = []\n",
        "            for subelem in elem:\n",
        "                image_size.append(int(subelem.text))\n",
        "            \n",
        "            info_dict['image_size'] = tuple(image_size)\n",
        "        \n",
        "        # Get details of the bounding box \n",
        "        elif elem.tag == \"object\":\n",
        "            bbox = {}\n",
        "            for subelem in elem:\n",
        "                if subelem.tag == \"name\":\n",
        "                    bbox[\"class\"] = subelem.text\n",
        "                    \n",
        "                elif subelem.tag == \"bndbox\":\n",
        "                    for subsubelem in subelem:\n",
        "                        bbox[subsubelem.tag] = int(float(subsubelem.text))           \n",
        "            info_dict['bboxes'].append(bbox)\n",
        "    \n",
        "    return info_dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqbJYFJJ8dxt"
      },
      "source": [
        "**Test Helper Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8Uudskp8iTa",
        "outputId": "e18593dd-a2f6-46b4-e365-1daa16891f96"
      },
      "source": [
        "print(extract_info_from_xml(\"/content/Data/Labels/set00/set00-occ_1.xml\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bboxes': [{'class': 'person', 'xmin': 46, 'ymin': 121, 'xmax': 172, 'ymax': 415}, {'class': 'person', 'xmin': 508, 'ymin': 166, 'xmax': 525, 'ymax': 216}, {'class': 'person', 'xmin': 432, 'ymin': 94, 'xmax': 543, 'ymax': 400}, {'class': 'person', 'xmin': 485, 'ymin': 162, 'xmax': 503, 'ymax': 215}, {'class': 'person', 'xmin': 585, 'ymin': 167, 'xmax': 602, 'ymax': 218}, {'class': 'person', 'xmin': 390, 'ymin': 117, 'xmax': 506, 'ymax': 353}], 'filename': 'set00-occ_1.jpg', 'image_size': (640, 480, 3)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW5MueXu_A7O"
      },
      "source": [
        "**Save the info dict to YOLO style and save to txt file**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hyDtdvm_AVS"
      },
      "source": [
        "# Dictionary that maps class names to IDs\n",
        "class_name_to_id_mapping = {\"person\": 0}\n",
        "\n",
        "# Convert the info dict to the required yolo format and write it to disk\n",
        "def convert_to_yolor(info_dict, output_path):\n",
        "    print_buffer = []\n",
        "\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    \n",
        "    # For each bounding box\n",
        "    for b in info_dict[\"bboxes\"]:\n",
        "        try:\n",
        "            class_id = class_name_to_id_mapping[b[\"class\"]]\n",
        "        except KeyError:\n",
        "            print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\n",
        "        \n",
        "        # Transform the bbox co-ordinates as per the format required by YOLO v5\n",
        "        b_center_x = (b[\"xmin\"] + b[\"xmax\"]) / 2 \n",
        "        b_center_y = (b[\"ymin\"] + b[\"ymax\"]) / 2\n",
        "        b_width    = (b[\"xmax\"] - b[\"xmin\"])\n",
        "        b_height   = (b[\"ymax\"] - b[\"ymin\"])\n",
        "        \n",
        "        # Normalise the co-ordinates by the dimensions of the image\n",
        "        image_w, image_h, image_c = info_dict[\"image_size\"]  \n",
        "        b_center_x /= image_w \n",
        "        b_center_y /= image_h \n",
        "        b_width    /= image_w \n",
        "        b_height   /= image_h \n",
        "        \n",
        "        #Write the bbox details to the file \n",
        "        print_buffer.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id, b_center_x, b_center_y, b_width, b_height))\n",
        "        \n",
        "    # Name of the file which we have to save \n",
        "    save_file_name = os.path.join(output_path, info_dict[\"filename\"].replace(\"jpg\", \"txt\"))\n",
        "    \n",
        "    # Save the annotation to disk\n",
        "    print(\"\\n\".join(print_buffer), file= open(save_file_name, \"w+\"))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRL0Ym2w_mRu"
      },
      "source": [
        "**Convert all XMLs to YOLO style bounding box txt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIRAgRSj_khL"
      },
      "source": [
        "input_path = '/content/Data/Labels'\n",
        "output_path = '/content/Data/Labels/Yolo'\n",
        "\n",
        "# Get the annotations\n",
        "sub_dirs = os.listdir(input_path)\n",
        "annotations = []\n",
        "for sub_dir in sub_dirs:\n",
        "    set_path = os.path.join(input_path, sub_dir)\n",
        "    outset_path = os.path.join(output_path, sub_dir)\n",
        "\n",
        "    if not os.path.exists(outset_path):\n",
        "        os.makedirs(outset_path)\n",
        "\n",
        "    annotation = [os.path.join(set_path, x) for x in os.listdir(set_path) if x[-3:] == \"xml\"]\n",
        "    for ann in annotation:\n",
        "        info_dict = extract_info_from_xml(ann)\n",
        "        convert_to_yolor(info_dict, outset_path)\n",
        "    annotations = annotations + [os.path.join(outset_path, x) for x in os.listdir(outset_path) if x[-3:] == \"txt\"]\n",
        "\n",
        "annotations.sort()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttjPYO5KDKMU"
      },
      "source": [
        "**Test drawing bounding box on images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNKDreWBDPe0"
      },
      "source": [
        "image_path = '/content/Data/Images'\n",
        "\n",
        "class_id_to_name_mapping = (dict(zip(class_name_to_id_mapping.values(), class_name_to_id_mapping.keys())))\n",
        "\n",
        "def plot_bounding_box(image, annotation_list):\n",
        "    annotations = np.array(annotation_list)\n",
        "    w, h = image.size\n",
        "    \n",
        "    plotted_image = ImageDraw.Draw(image)\n",
        "\n",
        "    transformed_annotations = np.copy(annotations)\n",
        "    transformed_annotations[:,[1,3]] = annotations[:,[1,3]] * w\n",
        "    transformed_annotations[:,[2,4]] = annotations[:,[2,4]] * h \n",
        "    \n",
        "    transformed_annotations[:,1] = transformed_annotations[:,1] - (transformed_annotations[:,3] / 2)\n",
        "    transformed_annotations[:,2] = transformed_annotations[:,2] - (transformed_annotations[:,4] / 2)\n",
        "    transformed_annotations[:,3] = transformed_annotations[:,1] + transformed_annotations[:,3]\n",
        "    transformed_annotations[:,4] = transformed_annotations[:,2] + transformed_annotations[:,4]\n",
        "    \n",
        "    for ann in transformed_annotations:\n",
        "        obj_cls, x0, y0, x1, y1 = ann\n",
        "        plotted_image.rectangle(((x0,y0), (x1,y1)))\n",
        "        \n",
        "        plotted_image.text((x0, y0 - 10), class_id_to_name_mapping[(int(obj_cls))])\n",
        "    \n",
        "    plt.imshow(np.array(image))\n",
        "    plt.show()\n",
        "\n",
        "# Get any random annotation file \n",
        "annotation_file = random.choice(annotations)\n",
        "with open(annotation_file, \"r\") as file:\n",
        "    annotation_list = file.read().split(\"\\n\")[:-1]\n",
        "    annotation_list = [x.split(\" \") for x in annotation_list]\n",
        "    annotation_list = [[float(y) for y in x ] for x in annotation_list]\n",
        "\n",
        "#Get the corresponding image file\n",
        "label_file_name = os.path.basename(annotation_file)\n",
        "\n",
        "image_file_token = label_file_name.split('-')\n",
        "image_dir = image_file_token[0]\n",
        "\n",
        "image_file = os.path.join(image_path, image_dir,label_file_name.replace(\"txt\", \"jpg\"))\n",
        "\n",
        "#print(image_file)\n",
        "assert os.path.exists(image_file)\n",
        "\n",
        "#Load the image\n",
        "image = Image.open(image_file)\n",
        "\n",
        "#Plot the Bounding Box\n",
        "plot_bounding_box(image, annotation_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixCFN1cu42xc"
      },
      "source": [
        "**Create folders for train/val/test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gTdtBJFI4eF"
      },
      "source": [
        "!mkdir /content/Data/YOLO\n",
        "!mkdir /content/Data/YOLO/images /content/Data/YOLO/labels\n",
        "!mkdir /content/Data/YOLO/images/train /content/Data/YOLO/images/val /content/Data/YOLO/images/test /content/Data/YOLO/labels/train /content/Data/YOLO/labels/val /content/Data/YOLO/labels/test"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uUwjPXeKMal"
      },
      "source": [
        "**Move data to corresponding folders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u0yIUTAJuOU"
      },
      "source": [
        "#Utility function to move images \n",
        "def move_files_to_folder(list_of_files, dst_dir):\n",
        "    for file in list_of_files:\n",
        "        try:\n",
        "            shutil.move(file, dst_dir)\n",
        "        except:\n",
        "            print(src)\n",
        "            assert False\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux59zqlqJ_cJ"
      },
      "source": [
        "# Read images and annotations\n",
        "image_src_dir = '/content/Data/Images'\n",
        "image_dst_dir = '/content/Data/YOLO/images'\n",
        "\n",
        "label_src_dir = '/content/Data/Labels/Yolo'\n",
        "label_dst_dir = '/content/Data/YOLO/labels'\n",
        "\n",
        "train_sets = ['set00', 'set01', 'set02', 'set03','set04','set05','set06']\n",
        "val_sets = ['set07']\n",
        "test_sets = ['set08']\n",
        "\n",
        "for train_set in train_sets:\n",
        "    train_images = [os.path.join(image_src_dir, train_set, x) for x in os.listdir(os.path.join(image_src_dir, train_set))]\n",
        "    move_files_to_folder(train_images, os.path.join(image_dst_dir, 'train'))\n",
        "    train_labels = [os.path.join(label_src_dir, train_set, x) for x in os.listdir(os.path.join(label_src_dir, train_set))]\n",
        "    move_files_to_folder(train_labels, os.path.join(label_dst_dir, 'train'))\n",
        "\n",
        "for val_set in val_sets:\n",
        "    train_images = [os.path.join(image_src_dir, val_set, x) for x in os.listdir(os.path.join(image_src_dir, val_set))]\n",
        "    move_files_to_folder(train_images, os.path.join(image_dst_dir, 'val'))\n",
        "    train_labels = [os.path.join(label_src_dir, val_set, x) for x in os.listdir(os.path.join(label_src_dir, val_set))]\n",
        "    move_files_to_folder(train_labels, os.path.join(label_dst_dir, 'val'))\n",
        "\n",
        "for test_set in test_sets:\n",
        "    train_images = [os.path.join(image_src_dir, test_set, x) for x in os.listdir(os.path.join(image_src_dir, test_set))]\n",
        "    move_files_to_folder(train_images, os.path.join(image_dst_dir, 'test'))\n",
        "    train_labels = [os.path.join(label_src_dir, test_set, x) for x in os.listdir(os.path.join(label_src_dir, test_set))]\n",
        "    move_files_to_folder(train_labels, os.path.join(label_dst_dir, 'test'))\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyjnD7So4WwG"
      },
      "source": [
        "**Create Data.yml and Data.names file in the same folder for YOLO training**\n",
        "\n",
        "Data.yml Example:\n",
        "\n",
        "```\n",
        "train: /content/Data/YOLO/images/train/\n",
        "val:  /content/Data/YOLO/images/val/\n",
        "test: /content/Data/YOLO/images/test/\n",
        "\n",
        "# number of classes\n",
        "nc: 1\n",
        "\n",
        "# class names\n",
        "names: [\"person\"]\n",
        "```\n",
        "\n",
        "Data.names example:\n",
        "\n",
        "```\n",
        "person\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMFxcjW95x3-"
      },
      "source": [
        "**YOLOR dependencies setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovqrY-xHaOYl"
      },
      "source": [
        "!git clone https://github.com/WongKinYiu/yolor\n",
        "%cd yolor\n",
        "\n",
        "# pip install required packages\n",
        "!pip install -qr requirements.txt\n",
        "\n",
        "# install mish-cuda if you want to use mish activation\n",
        "# https://github.com/thomasbrandon/mish-cuda\n",
        "# https://github.com/JunnYu/mish-cuda\n",
        "!git clone https://github.com/JunnYu/mish-cuda\n",
        "%cd mish-cuda\n",
        "!python setup.py build install\n",
        "%cd ..\n",
        "\n",
        "# install pytorch_wavelets if you want to use dwt down-sampling module\n",
        "# https://github.com/fbcotter/pytorch_wavelets\n",
        "!git clone https://github.com/fbcotter/pytorch_wavelets\n",
        "%cd pytorch_wavelets\n",
        "!pip install .\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBa4D9Hh4tVu"
      },
      "source": [
        "**Get pretrain implict knowledge, this is specific to YOLOR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Ro9t-gyh-P"
      },
      "source": [
        "%cd /content/yolor\n",
        "!bash scripts/get_pretrain.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsFSsGcD5FTl"
      },
      "source": [
        "**Start training with neural network architecture config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF2bs6_dwxcI",
        "outputId": "c417570e-a958-4b62-e8fe-a25eaedf8e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python train.py --batch-size 8 --img 640 640 --data '/content/Data/YOLO/Data.yml' --cfg cfg/yolor_p6.cfg --weights '/content/yolor/yolor_p6.pt' --device 0 --name yolor_pedestrian_detection --hyp '/content/yolor/data/hyp.finetune.1280.yaml' --epochs 50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 1.7.0 CUDA:0 (Tesla K80, 11441MB)\n",
            "\n",
            "Namespace(adam=False, batch_size=8, bucket='', cache_images=False, cfg='cfg/yolor_p6.cfg', data='/content/Data/YOLO/Data.yml', device='0', epochs=50, evolve=False, exist_ok=False, global_rank=-1, hyp='/content/yolor/data/hyp.finetune.1280.yaml', image_weights=False, img_size=[640, 640], local_rank=-1, log_imgs=16, multi_scale=False, name='yolor_pedestrian_detection', noautoanchor=False, nosave=False, notest=False, project='runs/train', rect=False, resume=False, save_dir='runs/train/yolor_pedestrian_detection6', single_cls=False, sync_bn=False, total_batch_size=8, weights='/content/yolor/yolor_p6.pt', workers=8, world_size=1)\n",
            "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
            "Hyperparameters {'lr0': 0.01, 'lrf': 0.2, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.5, 'scale': 0.8, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.2}\n",
            "Model Summary: 665 layers, 37265016 parameters, 37265016 gradients, 81.564040600 GFLOPS\n",
            "Transferred 862/862 items from /content/yolor/yolor_p6.pt\n",
            "Optimizer groups: 145 .bias, 145 conv.weight, 149 other\n",
            "Scanning labels /content/Data/YOLO/labels/train.cache3 (806 found, 0 missing, 0 empty, 286 duplicate, for 806 images): 806it [00:00, 7862.27it/s]\n",
            "Scanning labels /content/Data/YOLO/labels/val.cache3 (44 found, 0 missing, 0 empty, 24 duplicate, for 44 images): 44it [00:00, 2412.38it/s]\n",
            "NumExpr defaulting to 2 threads.\n",
            "Image sizes 640 train, 640 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/yolor_pedestrian_detection6\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      0/49     6.44G   0.04076   0.07191         0    0.1127        50       640: 100% 101/101 [02:55<00:00,  1.74s/it]\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      1/49      6.5G   0.03223   0.05587         0    0.0881        42       640: 100% 101/101 [02:30<00:00,  1.49s/it]\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      2/49      6.5G   0.02974   0.05166         0    0.0814       111       640: 100% 101/101 [02:28<00:00,  1.47s/it]\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      3/49      6.5G   0.03003   0.04754         0   0.07756        79       640: 100% 101/101 [02:28<00:00,  1.47s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:11<00:00,  3.82s/it]\n",
            "                 all          44         352       0.428       0.338       0.355       0.195\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      4/49     6.47G   0.03127   0.04948         0   0.08075        84       640: 100% 101/101 [02:28<00:00,  1.47s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:02<00:00,  1.37it/s]\n",
            "                 all          44         352       0.461        0.33       0.358       0.206\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      5/49     6.47G   0.03258   0.04784         0   0.08042       106       640: 100% 101/101 [02:27<00:00,  1.46s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:02<00:00,  1.37it/s]\n",
            "                 all          44         352       0.365       0.349       0.357       0.203\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      6/49     6.47G   0.03427   0.04684         0   0.08111       110       640:  40% 40/101 [00:58<01:28,  1.44s/it]"
          ]
        }
      ]
    }
  ]
}